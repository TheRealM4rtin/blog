| File Name | Title |
| --- | --- |
| Quantization Networks 1fbed10ce1974e43909f52f9c71e59fa.html | Quantization Networks |
| Fully Quantized Transformer for Machine Translatio cbd8aa5841b34bb3af29f67ceaf7b643.html | Fully Quantized Transformer for Machine Translation |
| LAMBADA 795ebd625d38428785849e27b9ace216.html | LAMBADA |
| MATH f8e23387608a44549e4213441ed3d3d1.html | MATH |
| Optimize Weight Rounding via Signed Gradient Desce 656c3a39baec42d798cabd3d06645511.html | Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs |
| SparseGPT Massive Language Models Can Be Accuratel a853d6404bdd490fb78ed8a16baa3c77.html | SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot |
| InfoPrompt Information-Theoretic Soft Prompt Tunin ce476471f55e4fbcb13960ff9007836c.html | InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding |
| TensorGPT Efficient Compression of the Embedding L 746390b9f94040f4a4226c3a3f21627d.html | TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition |
| GPT-4 feedback (GPT4) 7290f11974714ab99c1dafc2f4769f84.html | GPT-4 feedback (GPT4) |
| Extreme Language Model Compression with Optimal Su 542c6e85eab24a0fb25e5c195a4e5842.html | Extreme Language Model Compression with Optimal Subwords and Shared Projections |
| LightPAFF A Two-Stage Distillation Framework for P 9559a0bd5b6143529ff6f5a9806524ae.html | LightPAFF: A Two-Stage Distillation Framework for Pre-training and Fine-tuning |
| ARC (Challenge) de4e8bd655db4b1aa0c80291a833f65c.html | ARC (Challenge) |
| Introduction to weight quantization 9a422157fa584cf3921ea415165a7f2b.html | Introduction to weight quantization |
| LoftQ LoRA-Fine-Tuning-Aware Quantization for Larg ef5768877f194a1ca5d01160e650019a.html | LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models |
| DEEP COMPRESSION COMPRESSING DEEP NEURAL NETWORKS  9519bd3d65d8467c927998203a7c3078.html | DEEP COMPRESSION: COMPRESSING DEEP NEURAL
NETWORKS WITH PRUNING, TRAINED QUANTIZATION
AND HUFFMAN CODING |
| Compress, Then Prompt Improving Accuracy-Efficienc 234bc4bf253d4e1cafe2e87ac15462f7.html | Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt |
| Do Emergent Abilities Exist in Quantized Large Lan b046135419b342038e902e3ceecd8dc8.html | Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study |
| LoRAPrune Pruning Meets Low-Rank Parameter-Efficie f8d89e45d12e4447ab50022ceda412a7.html | LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning |
| Cost-Effective Hyperparameter Optimization for LLM f3bd606551b24f50b31f8b78f0f90f40.html | Cost-Effective Hyperparameter Optimization for LLMs Generation Inference |
| QIGen Generating Efficient Kernels for Quantized I 3a53eb80e62941c1a4d46e01c1694105.html | QIGen: Generating Efficient Kernels for Quantized Inference on Large Language Models |
| DistilBERT, a distilled version of BERT smaller, f 792c25ebcd18425599c25b7964856e0b.html | DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter |
| On the Distribution, Sparsity, and Inference-time  0bf98a3b150c4cef97e5f7ca4bee21a6.html | On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers |
| OliVe Accelerating Large Language Models via Hardw 2f9e415a573447efbabd64ea8592b843.html | OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization | Proceedings of the 50th Annual International Symposium on Computer Architecture |
| Compressing BERT Studying the Effects of Weight Pr 227a490163a74177b15978bf144e58de.html | Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning |
| BitNet Scaling 1-bit Transformers for Large Langua 1759c3dfe1ec4a1d8fc76fe137ca6086.html | BitNet: Scaling 1-bit Transformers for Large Language Models |
| QuIP 2-Bit Quantization of Large Language Models W ed72e94dd8944930a5d35f357ec4a876.html | QuIP: 2-Bit Quantization of Large Language Models With Guarantees |
| Explanations from Large Language Models Make Small 55a93c5a76da461f978cdce2e6f12d30.html | Explanations from Large Language Models Make Small Reasoners Better |
| Perplexity 83437c1710484c9eb983e0035c2b273d.html | Perplexity |
| Model Tells You What to Discard Adaptive KV Cache  81016008138749cdab0fa6b7838f6ccc.html | Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs |
| raw-WikiText2 8bd5b704dd434531a9f02e348ec1e283.html | raw-WikiText2 |
| OmniQuant Omnidirectionally Calibrated Quantizatio a8260cc91d5f4f718449b732f08549f7.html | OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models |
| TinyBERT Distilling BERT for Natural Language Unde 2bf41aee2da44dafb9f5131498c9e51a.html | TinyBERT: Distilling BERT for Natural Language Understanding |
| Fully Quantized Transformer for Improved Translati cc85adf260174abaa9cd4b6cdc97961b.html | Fully Quantized Transformer for Improved Translation |
| LLM-QAT Data-Free Quantization Aware Training for  b38abd5775284773bfcbcc3182aab114.html | LLM-QAT: Data-Free Quantization Aware Training for Large Language Models |
| ZeroQuant-FP A Leap Forward in LLMs Post-Training  1fe7afb53b7c4a3ab906ab3dc946def9.html | ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats |
| AD-KD Attribution-Driven Knowledge Distillation fo 3f1b90803ade4c9ca8ac330f856a564b.html | AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression |
| The Power of Scale for Parameter - Efficient Promp 102492c4360142458ebebcaa9c30e45a.html | The Power of Scale for Parameter - Efficient Prompt Tuning |
| Training Transformers with 4-bit Integers 22be34f6327345ae930025d244473ca9.html | Training Transformers with 4-bit Integers |
| Efficient 8-Bit Quantization of Transformer Neural 0a360ddb69534e51b26f99652f4b3565.html | Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model |
| ThuthfulQA (Multiple-Choice 2) 43ca70aa073448eeac94fb601c7f3ebd.html | ThuthfulQA (Multiple-Choice 2) |
| SQUEEZELLM Dense-and-sparse quantization 244c80c5b32a45ffb2a5673c97d45d01.html | SQUEEZELLM: Dense-and-sparse quantization |
| Junk DNA Hypothesis A Task-Centric Angle of LLM Pr 6e20249076dd4c3498d418d132f87d4e.html | Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity |
| VicunaEval 765c6bf63b8645168b9957d14b2f43df.html | VicunaEval |
| FlexRound Learnable Rounding based on Element-wise 55716a0aa6aa43ed969141c01f17e9b7.html | FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization |
| Distilling Step-by-Step! Outperforming Larger Lang d180ca59b7bd403c92cd0930ac712fc1.html | Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes |
| A Simple and Effective Pruning Approach for Large  7f5325dc8d704c6f89a86e233625cc53.html | A Simple and Effective Pruning Approach for Large Language Models |
| BoolQ dab679c508684d04a1f09f470ac0300d.html | BoolQ |
| Adaptative Budget Allocation for Parameter-Efficie 39b6641c22874b58bb4de00cfc57483d.html | Adaptative Budget Allocation for Parameter-Efficient Fine-Tuning (OPRO) |
| LUT-GEMM Quantized Matrix Multiplication based on  d16087d880104326a1f257bbe2989aac.html | LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models |
| Part 1 Eight Major Methods For FineTuning an LLM 2e9fa9cc746145b88051359530b7e3c3.html | Part 1: Eight Major Methods For FineTuning an LLM |
| INT-FP-QSim Mixed Precision and Formats For Large  3f61306c65fc4f0da006bdaed107246f.html | INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers |
| Sheared LLaMA Accelerating Language Model Pre-trai e4bf1c49bc96424f8ac7741c4fd7e7e8.html | Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning |
| QA-LoRA Quantization-Aware Low-Rank Adaptation of  510d662a881f402bb51608a960211658.html | QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models |
| Democratizing Reasoning Ability Tailored Learning  ade5646e9714438088cc0bc8e2eaa5e1.html | Democratizing Reasoning Ability: Tailored Learning from Large Language Model |
| Prune and Tune Improving Efficient Pruning Techniq c90654626f4a41f3856c6f4f1c1061e6.html | Prune and Tune: Improving Efficient Pruning Techniques for Massive... |
| QuantEase Optimization-based Quantization for Lang fb683fb8109d418b8990d311a9dfbc27.html | QuantEase: Optimization-based Quantization for Language Models -- An Efficient and Intuitive Algorithm |
| Parameter-efficient fine-tuning of large-scale pre 6c6d1191b72a46558ad3cf4dee4346b0.html | Parameter-efficient fine-tuning of large-scale pre-trained language models - Nature Machine Intelligence |
| Medium 4e8a7fdd97984010be509ad79898f5cf.html | Medium |
| SelfInst da8e46da20394a7683c5fb2ab76951ea.html | SelfInst |
| APPS f2b78c4284b24030b6f5f0104207321f.html | APPS |
| eDKM An Efficient and Accurate Train-time Weight C 329217dbcf4a49bb96ec62d4fbb7090f.html | eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models |
| Quantize Llama models with ggml b565307de62a43a9a8020b1e795bdc83.html | Quantize Llama models with ggml |
| Exploring the Limits of Transfer Learning with a U 7a1d5c30ce4344fda2423cc7343be507.html | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer |
| Untitled 6f80e9d2446d48cb902b02437320f495.html | Untitled |
| One-Shot Sensitivity-Aware Mixed Sparsity Pruning  08961420e1714fdcbe6490769ac903c3.html | One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models |
| Compressing Large-Scale Transformer-Based Models A 547892f1c84244c69951fbf44d5e176c.html | Compressing Large-Scale Transformer-Based Models: A Case Study on BERT |
| Flash-LLM Enabling Cost-Effective and Highly-Effic c19a7ad47ee8451a81ace5843f5d2eec.html | Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity |
| Generalized Knowledge Distillation for Auto-regres 34bf8482490046af8039f84f3a65379f.html | Generalized Knowledge Distillation for Auto-regressive Language Models |
| LLM-Pruner On the Structural Pruning of Large Lang 60f80024ebbb43f5a51c4d61d3b888ca.html | LLM-Pruner: On the Structural Pruning of Large Language Models |
| E-Sparse Boosting the Large Language Model Inferen d784f6fc9c2f4c8cb34a66869b29b6fc.html | E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity |
| NF4 Isn’t Information Theoretically Optimal (and t cc5e01a9a9f44f23962a265913af532b.html | NF4 Isn’t Information Theoretically Optimal (and that’s Good) |
| Deja Vu Contextual Sparsity for Efficient LLMs at  f48cf2259d93495ca11e8255ff88bbc0.html | Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time Oral |
| BART Denoising Sequence-to-Sequence Pre-training f d998fc3e9d15425e9fae5f79b2258c9b.html | BART: Denoising Sequence-to-Sequence Pre-training for Natural
Language Generation, Translation, and Comprehension |
| Large Language Models as Optimizers c7dc6715ca624bbb8cb9b3c781c6b399.html | Large Language Models as Optimizers |
| XSUM 5bf5edb181f34e6996a074b24af2aa84.html | XSUM |
| Optimal Brain Surgeon and general network pruning 323c7df69037495d90a34334d0f658f8.html | Optimal Brain Surgeon and general network pruning |
| Prefix-Tuning Optimizing Continuous Prompts for Ge 1ee0ec5df74d430caf950faf7823f2f6.html | Prefix-Tuning: Optimizing Continuous Prompts for Generation |
| Local Large Language Models 6446195d0d934b1aa6288bf9e478dd45.html | Local Large Language Models |
| LongLoRA Efficient Fine-tuning of Long-Context Lar 5425cd2c3b884a1cbf6a9b1c0d4b601f.html | LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models |
| Revisiting Block-based Quantisation What is Import 3815253da7c24b93b8f5bac282d6c485.html | Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference? |
| The Lazy Neuron Phenomenon On Emergence of Activat 53856ef53a864758bd89390afa53f9ee.html | The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers |
| 4-bit Quantization with GPTQ e0913b46fae241b2972a17c0bf4dd68c.html | 4-bit Quantization with GPTQ |
| PIQA c2404668feed43c19a7dcecd857e85c2.html | PIQA |
| Token-Scaled Logit Distillation for Ternary Weight 4e84e26551354ecab485efdf4e7e5b6b.html | Token-Scaled Logit Distillation for Ternary Weight Generative Language Models |
| Dynamic Sparse No Training Training-Free Fine-tuni a04c9d3646eb4c3ab4ef8940c70d653a.html | Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs |
| Understanding the Impact of Post-Training Quantiza 7ffa0b6353d34c66b3f19849880726a4.html | Understanding the Impact of Post-Training Quantization on Large Language Models |
| ModuLoRA Finetuning 3-Bit LLMs on Consumer GPUs by e4e5371ce77a4c3182908f2aff4dc8b0.html | ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers |
| Mistral 7B 54c61312629f465d9af3ecac455131ae.html | Mistral 7B |
| Dual Grained Quantization Efficient Fine-Grained Q 8d854a4de580451d9aa77fd27f9fe361.html | Dual Grained Quantization: Efficient Fine-Grained Quantization for LLM |
| SpQR A Sparse-Quantized Representation for Near-Lo 19439c7f7ea1477581e857f3a7ad280a.html | SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression |
| In-context Learning Distillation Transferring Few- a219280a709b4d4ab62b8c5134738e82.html | In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models |
| MiniLMv2 Multi-Head Self-Attention Relation Distil 2afdb8785fe94f32b4b2e2c705d18028.html | MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers |
| LLMLingua Compressing Prompts for Accelerated Infe 746c10749999432d8bdf78fd60219f54.html | LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models |
| Low-Rank Prune-And-Factorize for Language Model Co 2ddbcfd6462143d395210567b468148c.html | Low-Rank Prune-And-Factorize for Language Model Compression |
| (mBART) Multilingual Denoising Pre-training for Ne f816cfb3b0e147f4ba16f75e078386dc.html | (mBART) Multilingual Denoising Pre-training for Neural Machine Translation |
| ExAcc 7097dc276a564f00bf202467aaf5099c.html | ExAcc |
| Large Language Models Are Reasoning Teachers 5bb8219f1e994df7a49934e6e2f02911.html | Large Language Models Are Reasoning Teachers |
| Towards Efficient Post-training Quantization of Pr 1da80007bc344c20b7430f2e9b2fe822.html | Towards Efficient Post-training Quantization of Pre-trained Language Models |
| Knowledge Distillation of Large Language Models 523ac8fa3a0d4826a4f3ece18c65b1ca.html | Knowledge Distillation of Large Language Models |
| Structural Pruning of Large Language Models via Ne 64d16f6c14814bcda4323cd89adfbbee.html | Structural Pruning of Large Language Models via Neural Architecture... |
| LaMini-LM A Diverse Herd of Distilled Models from  0428f1140d0b422a8d8d14a3248585ad.html | LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions |
| Understanding INT4 Quantization for Transformer Mo 72e01826c261449a83e3ced5f465fd75.html | Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases |
| Gradient-Based Post-Training Quantization Challeng 32408e10a0a24fe9adffff82c49fa70e.html | Gradient-Based Post-Training Quantization: Challenging the Status Quo |
| Memory-Efficient Fine-Tuning of Compressed Large L 699fefce5e8a4722aef71196093c0278.html | Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization |
| UnNI 3d31dabad75a44eaaa2a28836344552c.html | UnNI |
| QLLM Accurate and Efficient Low-Bitwidth Quantizat e0278b16716a48649941262aa7e0ccd9.html | QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models |
| Rouge-L (R-L) d82b376d38894af19ed115ca88f08950.html | Rouge-L (R-L) |
| SCOTT Self-Consistent Chain-of-Thought Distillatio 56e021af97aa4db891f91144de24de80.html | SCOTT: Self-Consistent Chain-of-Thought Distillation |
| I-BERT Integer-only BERT Quantization 267f9e400d5945b7bb60ebfb9e0687cd.html | I-BERT: Integer-only BERT Quantization |
| Do Not Blindly Imitate the Teacher Using Perturbed 988db861be9340f887607f9110b6a9f0.html | Do Not Blindly Imitate the Teacher: Using Perturbed Loss for Knowledge Distillation |
| Boost Transformer-based Language Models with GPU-F 0eabdf8c7927447797ed2382df8fe2b0.html | Boost Transformer-based Language Models with GPU-Friendly Sparsity and Quantization |
| BinaryBERT Pushing the Limit of BERT Quantization d5327e14367b46e69215ca604edc9035.html | BinaryBERT: Pushing the Limit of BERT Quantization |
| Bibliography 6e282d1b48a7492d93902b4f74b315f5.html | Bibliography |
| GPT-Zip Deep Compression of Finetuned Large Langua 9bf807d61b5e406b804da3e733939311.html | GPT-Zip: Deep Compression of Finetuned Large Language Models |
| PreQuant A Task-agnostic Quantization Approach for 90907549b8724e5eac2717b3ff456f2c.html | PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models |
| Quantizable Transformers Removing Outliers by Help bff3ca78ad6043fb8358a2ccb0331459.html | Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing |
| “Low-Resource” Text Classification A Parameter-Fre 2616fa1cc7054919b4dc240eebb647d5.html | “Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors |
| What Matters In The Structured Pruning of Generati c906aeacfe5e4e988f16baa5be303600.html | What Matters In The Structured Pruning of Generative Language Models? |
| MEMORY-VQ Compression for Tractable Internet-Scale 9e794ac43ca945adb829a6d8ffb0ab92.html | MEMORY-VQ: Compression for Tractable Internet-Scale Memory |
| Optimal Brain Compression A Framework for Accurate 8f880d5de93346d281678d75c29e2d92.html | Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning |
| FPTQ Fine-grained Post-Training Quantization for L 519ebfc92f644278aeacbb4a70a5e825.html | FPTQ: Fine-grained Post-Training Quantization for Large Language Models |
| RPTQ Reorder-based Post-training Quantization for  dd2a352d372c4801b9b49e2383aa10c6.html | RPTQ: Reorder-based Post-training Quantization for Large Language Models |
| OWQ Lessons learned from activation outliers for w 1d65e62429a94503ba6e41ec2963f19c.html | OWQ: Lessons learned from activation outliers for weight quantization in large language models |
| LoRa 5ce116ff34044473809964b36950177e.html | LoRa |
| A Survey on Model Compression for Large Language M 7dbf0da37be5482db1d2f7d4f60a4ffe.html | A Survey on Model Compression for Large Language Models |
| SST2 ca072d3c4b594c769f340b5c5c442b98.html | SST2 |
| INT2 1 Towards Fine-Tunable Quantized Large Langua 9ee48212f36a4a54aff48bf829e3e7e9.html | INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation |
| LLM int8() 8-bit Matrix Multiplication for Transfo 61627522fa6d4bdea3108314a639cf9a.html | LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale |
| Importance Estimation for Neural Network Pruning e224f3d4d8954ff6b4218e1ec5299b6d.html | Importance Estimation for Neural Network Pruning |
| LLM int8() 8-bit Matrix Multiplication for Transfo fb18006eb5e840a29762c6bd9f18471b.html | LLM.int8(): 8-bit Matrix Multiplication
for Transformers at Scale |
| Symbolic Chain-of-Thought Distillation Small Model 6f5ef8f2d4ba43f89f2c6a85edc947c9.html | Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step |
| HellaSwag 944759d1f12646ceb9f7447665d08ed7.html | HellaSwag |
| Efficient Fine-Tuning with LoRA A Guide to Optimal 235d192c375843d887338327756d6563.html | Efficient Fine-Tuning with LoRA: A Guide to Optimal Parameter Selection for Large Language Models |
| Quantized Distributed Training of Large Models wit a018b2ce5094436e8d4c398c947d7db6.html | Quantized Distributed Training of Large Models with Convergence Guarantees |
| RoBERTa A Robustly Optimized BERT Pretraining Appr cfb855483443498abe481ea9755eccb8.html | RoBERTa: A Robustly Optimized BERT Pretraining Approach |
| GLUE A MULTI-TASK BENCHMARK AND ANALYSIS PLATFORM  d198d805f32b4a5c86c0fe3e1676b6f4.html | GLUE: A MULTI-TASK BENCHMARK AND ANALYSIS
PLATFORM FOR NATURAL LANGUAGE UNDERSTANDING |
| Integer or Floating Point New Outlooks for Low-Bit ba45ea8adf3b4ea68bf885364e43b754.html | Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models |
| BERT Pre-training of Deep Bidirectional Transforme 1ff253752a8340eeaf01a5832218ac39.html | BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding |
| Compresso Structured Pruning with Collaborative Pr adef6c9f2c7940b48aa4d29029684653.html | Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models |
| AWQ Activation-aware Weight Quantization for LLM C a5b30bb4bab348e885bb0db416f543f1.html | AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration |
| Outlier Suppression+ Accurate quantization of larg 260009b9b6c148c8b5628d734a245be8.html | Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling |
| PB-LLM Partially Binarized Large Language Models a97452bf186a48f583790cac72910ab6.html | PB-LLM: Partially Binarized Large Language Models |
| LoSparse Structured Compression of Large Language  e9de92aebf014fb98bc3b70b72371bb3.html | LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation |
| Variational Convolutional Neural Network Pruning 2b3d9bb8c92f40dcacbe8494163caa71.html | Variational Convolutional Neural Network Pruning |
| Q8BERT Quantized 8Bit BERT 99321c930c2f4c8db62ee54e7731e29b.html | Q8BERT: Quantized 8Bit BERT |
| ZeroQuant-V2 Exploring Post-training Quantization  a8f22961e4e343c8af4fd7fab05a6bdb.html | ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation |
| HumanEval f86a1f95448a48ac8ad49d97361840cc.html | HumanEval  |
| S-NI e0a3e2a680794a718cfb41c6ebeb13ae.html | S-NI |
| Compression of Generative Pre-trained Language Mod b5a13cd07e674aa2b68d0051f3005b1a.html | Compression of Generative Pre-trained Language Models via Quantization |
| Q-BERT Hessian Based Ultra Low Precision Quantizat 25fd728a69a44a1d84353f266f570d89.html | Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT |
| MiniLM Deep Self-Attention Distillation for Task-A db059afdb0264faa8b8cc917eb62667d.html | MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers |
| COMPRESSING LLMS THE TRUTH IS RARELY PURE AND NEVE 223b65a12ec4471e8504bcce3e364139.html | COMPRESSING LLMS: THE TRUTH IS RARELY PURE
AND NEVER SIMPLE |
| TernaryBERT Distillation-aware Ultra-low Bit BERT 0cb857bb294c414682e447f2e7ae6ed5.html | TernaryBERT: Distillation-aware Ultra-low Bit BERT |
| HyperAttention Long-context Attention in Near-Line 54c156226a06440185c65eff44fad8a0.html | HyperAttention: Long-context Attention in Near-Linear Time |
| Distilling Reasoning Capabilities into Smaller Lan 8a5883eaeb294813abe5d3c99cdd167c.html | Distilling Reasoning Capabilities into Smaller Language Models |
| Efficient Post-training Quantization with FP8 Form 25b97092ee7a42aba0cd1b629a67be04.html | Efficient Post-training Quantization with FP8 Formats |
| ECE 827fde5da7a446aaaa706500e6b16a01.html | ECE |
| SmoothQuant Accurate and Efficient Post-Training Q 4e58f80d2337454e9f17f0a604fb3058.html | SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models |
| Towards Fully 8-bit Integer Inference for the Tran f4d77eab2cef445fbb566c186e6d6e6f.html | Towards Fully 8-bit Integer Inference for the Transformer Model |
| ALBERT A LITE BERT FOR SELF-SUPERVISED LEARNING OF b18545498f20403dad774522903d82d0.html | ALBERT: A LITE BERT FOR SELF-SUPERVISED
LEARNING OF LANGUAGE REPRESENTATIONS |
| One-Shot Sensitivity-Aware Mixed Sparsity Pruning  c7a35419197743f893466ba11d96e2ef.html | One-Shot Sensitivity-Aware Mixed Sparsity Pruning for LLMs |
| Outlier Weighed Layerwise Sparsity (OWL) A Missing 499a38a9fbd740fba50c0e3516f8fb93.html | Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity |
| GPTQ Accurate Post-Training Quantization for Gener 7e947c0330bc47859b94a2f6fa044114.html | GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers |
| What is the State of Neural Network Pruning e116ca1b11ac4550981efe6371c45e41.html | What is the State of Neural Network Pruning? |
| NUPES Non-Uniform Post-Training Quantization via P 455284b5a1f946669bd4498891e8bc08.html | NUPES : Non-Uniform Post-Training Quantization via Power Exponent Search |
| FP8-LM Training FP8 Large Language Models 975e9d3adedc4a758b0df312cec0c76a.html | FP8-LM: Training FP8 Large Language Models |
| SuperGLUE A Stickier Benchmark for General-Purpose cee7d128756f4635aa69e76ecd211b3a.html | SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems |
| QLORA Efficient Finetuning of Quantized LLMs 5b856e9e1da14fb8af755b046abdaf60.html | QLORA: Efficient Finetuning of Quantized LLMs |
| GACT Activation Compressed Training for Generic Ne b190a4ba766649d2aa000a67f35a8599.html | GACT: Activation Compressed Training for Generic Network Architectures |
| FineQuant Unlocking Efficiency with Fine-Grained W 7f07d48465b04692808218238eaa99ad.html | FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs |
| TEQ Trainable Equivalent Transformation for Quanti 4d40dfb7bb554945891eb8c05d0462c7.html | TEQ: Trainable Equivalent Transformation for Quantization of LLMs |
| QFT Quantized Full-parameter Tuning of LLMs with A 47be9ff8e35b4d5eadf71bc1020e1fa9.html | QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources |
| Integer Quantization for Deep Learning Inference P 47b068a5efbc4ec6930b4133b4a549a1.html | Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation |
| PTB aebca5518bf24fd5a18e01385fc2dd19.html | PTB |
| A Survey of Quantization Methods for Efficient Neu ddfc83acee894d769ab52f99bd0696e7.html | A Survey of Quantization Methods for Efficient Neural Network Inference |
| Rethinking Channel Dimensions to Isolate Outliers  69bddb923da94e9cb383985415adeb54.html | Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models |
| GOBO Quantizing Attention-Based NLP Models for Low 9f064a904abe4531b0bcb66d1fe62ae2.html | GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference |
| GPTQ Accurate Post-Training Quantization for Gener 370adda760ad4b9e999743596802a358.html | GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers |
| DollyEval 8edbf44dc412472f9df64e27f170a36b.html | DollyEval |
| Zero-Shot Sharpness-Aware Quantization for Pre-tra 0716a2abcde049f0bc92d16e11c4b768.html | Zero-Shot Sharpness-Aware Quantization for Pre-trained Language Models |
| EleutherAI-evalharness 74172089008b40099f39323b75045666.html | EleutherAI-evalharness |
| HELM 28e630ded8be4d46aba1465a3775e248.html | HELM |
| MCC-KD Multi-CoT Consistent Knowledge Distillation e9a5467b934c4dadac7db16a4b6f8bd4.html | MCC-KD: Multi-CoT Consistent Knowledge Distillation |
| ZeroQuant Efficient and Affordable Post-Training Q 444b3be3928b4bef911712728ad691ad.html | ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers |
| A Fast Post-Training Pruning Framework for Transfo 9a3a6d869d094d9aa021411ac4a5cc65.html | A Fast Post-Training Pruning Framework for
Transformers |
| BiBERT Accurate Fully Binarized BERT b407ce5c3aac441686cdc1355ec2ac76.html | BiBERT: Accurate Fully Binarized BERT |
| Pruning Large Language Models via Accuracy Predict f191f19776aa44b0a40fa7ff2305c015.html | Pruning Large Language Models via Accuracy Predictor |
| LLM-FP4 4-Bit Floating-Point Quantized Transformer 31cbdf1bce3145ce90de2bb718a00648.html | LLM-FP4: 4-Bit Floating-Point Quantized Transformers |
| ZeRO Memory Optimizations Toward Training Trillion 2ecb40bd56b547099e9fb0c47f00ad59.html | ZeRO: Memory Optimizations Toward Training Trillion
Parameter Models |
| Cost-effective Distillation of Large Language Mode 9cb68428445540798cafcfe64a31d3db.html | Cost-effective Distillation of Large Language Models |
| Lion Adversarial Distillation of Proprietary Large ee397b77d76c464a8155f6fa2476892a.html | Lion: Adversarial Distillation of Proprietary Large Language Models |
| Norm Tweaking High-performance Low-bit Quantizatio 15e6dbd36d724afcbd4355678a310468.html | Norm Tweaking: High-performance Low-bit Quantization of Large Language Models |
| C4 4e4bb4abd49a4b22b11286547a28f818.html | C4 |
| Large Transformer Model Inference Optimization a8baf79fd9e24ee38f34b5126e164031.html | Large Transformer Model Inference Optimization |
