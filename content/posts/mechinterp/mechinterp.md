---
title: Mechanistic Interpretability
date: "2024-02-28"
draft: false
tags: ['LLMs', 'Papers', 'State-of-The-Art']
categories: ['research']
---

"Mechanistic Interpretability is the study of reverse-engineering neural networks - analogous to how we might try to reverse-engineer a programâ€™s source code from its compiled binary, our goal is to reverse engineer the parameters of a trained neural network, and to try to reverse engineer what algorithms and internal cognition the model is actually doing. Going from knowing that it works, to understanding how it works."


> https://www.neelnanda.io/mechanistic-interpretability/quickstart

## An Introduciton to Circuits

> https://distill.pub/2020/circuits/zoom-in/


## Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability
> https://arxiv.org/pdf/2402.10688.pdf

