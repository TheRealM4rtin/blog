<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>LLMs Compression | Literature Overview | Our work | TheRealMartin&#39;s Blog</title>
<meta name="keywords" content="Papers, State-of-The-Art, LLMs">
<meta name="description" content="TL;DR I&rsquo;m currently working on a research project on LLMs Compression. Making these models smaller and more efficient. This includes techniques like Pruning, Quantization, and Distillation. The paper will be published mid-2024.
During our research, we gathered over 150 research papers related to state-of-the-art LLMs Compression methods and Optimization. I think this collection is pretty valuable. So here&rsquo;s this massive pile of research for you.
Our work In our research, we address the challenging task of compressing large language models (LLMs) using advanced compression techniques.">
<meta name="author" content="">
<link rel="canonical" href="https://therealm4rtin.github.io/blog/posts/literature-overview/post-sota-llm/">
<link crossorigin="anonymous" href="/blog/assets/css/stylesheet.e087fd1dc76e73a35ae6d7028ddc1ba41e0131e7f9b3a6e2d019a208e6d6c4b5.css" integrity="sha256-4If9Hcduc6Na5tcCjdwbpB4BMef5s6bi0BmiCObWxLU=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/blog/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://therealm4rtin.github.io/blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://therealm4rtin.github.io/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://therealm4rtin.github.io/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://therealm4rtin.github.io/blog/apple-touch-icon.png">
<link rel="mask-icon" href="https://therealm4rtin.github.io/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="LLMs Compression | Literature Overview | Our work" />
<meta property="og:description" content="TL;DR I&rsquo;m currently working on a research project on LLMs Compression. Making these models smaller and more efficient. This includes techniques like Pruning, Quantization, and Distillation. The paper will be published mid-2024.
During our research, we gathered over 150 research papers related to state-of-the-art LLMs Compression methods and Optimization. I think this collection is pretty valuable. So here&rsquo;s this massive pile of research for you.
Our work In our research, we address the challenging task of compressing large language models (LLMs) using advanced compression techniques." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://therealm4rtin.github.io/blog/posts/literature-overview/post-sota-llm/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-25T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-01-25T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="LLMs Compression | Literature Overview | Our work"/>
<meta name="twitter:description" content="TL;DR I&rsquo;m currently working on a research project on LLMs Compression. Making these models smaller and more efficient. This includes techniques like Pruning, Quantization, and Distillation. The paper will be published mid-2024.
During our research, we gathered over 150 research papers related to state-of-the-art LLMs Compression methods and Optimization. I think this collection is pretty valuable. So here&rsquo;s this massive pile of research for you.
Our work In our research, we address the challenging task of compressing large language models (LLMs) using advanced compression techniques."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://therealm4rtin.github.io/blog/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "LLMs Compression | Literature Overview | Our work",
      "item": "https://therealm4rtin.github.io/blog/posts/literature-overview/post-sota-llm/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LLMs Compression | Literature Overview | Our work",
  "name": "LLMs Compression | Literature Overview | Our work",
  "description": "TL;DR I\u0026rsquo;m currently working on a research project on LLMs Compression. Making these models smaller and more efficient. This includes techniques like Pruning, Quantization, and Distillation. The paper will be published mid-2024.\nDuring our research, we gathered over 150 research papers related to state-of-the-art LLMs Compression methods and Optimization. I think this collection is pretty valuable. So here\u0026rsquo;s this massive pile of research for you.\nOur work In our research, we address the challenging task of compressing large language models (LLMs) using advanced compression techniques.",
  "keywords": [
    "Papers", "State-of-The-Art", "LLMs"
  ],
  "articleBody": "TL;DR I’m currently working on a research project on LLMs Compression. Making these models smaller and more efficient. This includes techniques like Pruning, Quantization, and Distillation. The paper will be published mid-2024.\nDuring our research, we gathered over 150 research papers related to state-of-the-art LLMs Compression methods and Optimization. I think this collection is pretty valuable. So here’s this massive pile of research for you.\nOur work In our research, we address the challenging task of compressing large language models (LLMs) using advanced compression techniques. The essence of our study is to demonstrate the practicality and real-world applicability of this challenge. While existing literature suggests remarkable success in LLM compression, our study is grounded in real-world applications, seeking to determine the true value of these methods in practical settings.\nMemory usage is a critical issue in LLMs. When a model requires more memory, it leads to increased data movement. This, in turn, results in higher energy consumption. The more a system has to access memory, the more energy it uses. Therefore, reducing the memory footprint of LLMs is not just about saving space; it’s also about energy efficiency and faster processing.\nThe core contributions of our study are:\nInnovative Combinations We have innovatively combined state-of-the-art compression techniques in a way that, as far as we know, hasn’t been attempted before.\nPractical Measures We have evaluated our models in the context of real-world applicability. Beyond theoretical measures such as perplexity, we use LLM-Kick, a practical application for LLMs, to measure performance in a real production environment.\nOur focus isn’t just on theoretical advancements but on the tangible impact of compression in deployment and development. Through this approach, we aim to provide a straightforward analysis of LLM compression, cutting through the hype to reveal its true value in everyday use.\nTechniques: Quantization Quantization reduces the number of bits required to represent each weight in the model, thus decreasing its overall size. Knowledge Distillation Knowledge Distillation effectively condensing the knowledge by training a smaller model (student) to replicate the performance of a larger one (teacher). Pruning Pruning involves removing less important parts of the model to\nmake it more efficient without significant loss in performance. Right now, the collection is organized as a Notion webpage that I’ve made accessible. I’m still considering other platforms like Zotero or GitHub to host this library, but I haven’t settled on one yet. If you have any suggestions or preferences, let me know. Update, I will do a HF-Collection.\nHugging-Face Collection (in progress) https://huggingface.co/collections/TheRealM4rtin/llms-compression-65c368e270ee73487f38b45d\nNotion Collection https://therealmartin.notion.site/LLM-Compression-A-state-of-the-art-6a920ba022c54f0ab4e47b0654e7b738?pvs=4\nNote that if you see a green check emoji ✅ next to a paper on the Notion page, it means there’s a brief report available for that paper right there on the page.\nSome themes covered Quantization Pruning Distillation Low-Rank Factorization Compression Finetuning Hyperparameters Encoding Prompt Benchmarks Open Source Models Agents Memory ",
  "wordCount" : "472",
  "inLanguage": "en",
  "datePublished": "2024-01-25T00:00:00Z",
  "dateModified": "2024-01-25T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://therealm4rtin.github.io/blog/posts/literature-overview/post-sota-llm/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TheRealMartin's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://therealm4rtin.github.io/blog/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://therealm4rtin.github.io/blog/" accesskey="h" title="TheRealMartin&#39;s Blog (Alt + H)">
                <img src="https://therealm4rtin.github.io/blog/images/favicon.png" alt="" aria-label="logo"
                    height="30">TheRealMartin&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://therealm4rtin.github.io/blog/archives/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://therealm4rtin.github.io/blog/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://therealm4rtin.github.io/blog/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://therealm4rtin.github.io/blog/about/" title="About Me">
                    <span>About Me</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://therealm4rtin.github.io/blog/">Home</a>&nbsp;»&nbsp;<a href="https://therealm4rtin.github.io/blog/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      LLMs Compression | Literature Overview | Our work
    </h1>
    <div class="post-meta"><span title='2024-01-25 00:00:00 +0000 UTC'>January 25, 2024</span>&nbsp;·&nbsp;3 min

</div>
  </header> 
  <div class="post-content"><p><strong><em>TL;DR</em></strong> I&rsquo;m currently working on a research project on LLMs Compression. Making these models smaller and more efficient. This includes techniques like Pruning, Quantization, and Distillation. The paper will be published mid-2024.</p>
<p>During our research, we gathered over 150 research papers related to state-of-the-art LLMs Compression methods and Optimization. I think this collection is pretty valuable. So here&rsquo;s this massive pile of research for you.</p>
<h2 id="our-work">Our work<a hidden class="anchor" aria-hidden="true" href="#our-work">#</a></h2>
<p>In our research, we address the challenging task of compressing large language models (LLMs) using advanced compression techniques. The essence of our study is to demonstrate the practicality and real-world applicability of this challenge. While existing literature suggests remarkable success in LLM compression, our study is grounded in real-world applications, seeking to determine the true value of these methods in practical settings.</p>
<p>Memory usage is a critical issue in LLMs. When a model requires more memory, it leads to increased data movement. This, in turn, results in higher energy consumption. The more a system has to access memory, the more energy it uses. Therefore, reducing the memory footprint of LLMs is not just about saving space; it&rsquo;s also about energy efficiency and faster processing.</p>
<p>The core contributions of our study are:</p>
<h4 id="innovative-combinations">Innovative Combinations<a hidden class="anchor" aria-hidden="true" href="#innovative-combinations">#</a></h4>
<p>We have innovatively combined state-of-the-art compression techniques in a way that, as far as we know, hasn&rsquo;t been attempted before.</p>
<h4 id="practical-measures">Practical Measures<a hidden class="anchor" aria-hidden="true" href="#practical-measures">#</a></h4>
<p>We have evaluated our models in the context of real-world applicability. Beyond theoretical measures such as perplexity, we use LLM-Kick, a practical application for LLMs, to measure performance in a real production environment.</p>
<p>Our focus isn&rsquo;t just on theoretical advancements but on the tangible impact of compression in deployment and development. Through this approach, we aim to provide a straightforward analysis of LLM compression, cutting through the hype to reveal its true value in everyday use.</p>
<h3 id="techniques">Techniques:<a hidden class="anchor" aria-hidden="true" href="#techniques">#</a></h3>
<h4 id="quantization">Quantization<a hidden class="anchor" aria-hidden="true" href="#quantization">#</a></h4>
<ul>
<li>Quantization reduces the number of bits required to represent each weight in the model, thus decreasing its overall size.</li>
</ul>
<h4 id="knowledge-distillation">Knowledge Distillation<a hidden class="anchor" aria-hidden="true" href="#knowledge-distillation">#</a></h4>
<ul>
<li>Knowledge Distillation effectively condensing the knowledge by training a smaller model (student) to replicate the performance of a larger one (teacher).</li>
</ul>
<h4 id="pruning">Pruning<a hidden class="anchor" aria-hidden="true" href="#pruning">#</a></h4>
<ul>
<li>Pruning involves removing less important parts of the model to<br>
make it more efficient without significant loss in performance.</li>
</ul>
<hr>
<p>Right now, the collection is organized as a Notion webpage that I&rsquo;ve made accessible. I&rsquo;m still considering other platforms like Zotero or GitHub to host this library, but I haven&rsquo;t settled on one yet. If you have any suggestions or preferences, <em>let me know</em>. Update, I will do a HF-Collection.</p>
<h4 id="hugging-face-collection-in-progress">Hugging-Face Collection (in progress)<a hidden class="anchor" aria-hidden="true" href="#hugging-face-collection-in-progress">#</a></h4>
<blockquote>
<p><a href="https://huggingface.co/collections/TheRealM4rtin/llms-compression-65c368e270ee73487f38b45d">https://huggingface.co/collections/TheRealM4rtin/llms-compression-65c368e270ee73487f38b45d</a></p>
</blockquote>
<h4 id="notion-collection">Notion Collection<a hidden class="anchor" aria-hidden="true" href="#notion-collection">#</a></h4>
<blockquote>
<p><a href="https://therealmartin.notion.site/LLM-Compression-A-state-of-the-art-6a920ba022c54f0ab4e47b0654e7b738?pvs=4">https://therealmartin.notion.site/LLM-Compression-A-state-of-the-art-6a920ba022c54f0ab4e47b0654e7b738?pvs=4</a></p>
</blockquote>
<p>Note that if you see a green check emoji ✅ next to a paper on the Notion page, it means there&rsquo;s a brief report available for that paper right there on the page.</p>
<hr>
<h2 id="some-themes-covered">Some themes covered<a hidden class="anchor" aria-hidden="true" href="#some-themes-covered">#</a></h2>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Quantization</td>
<td>Pruning</td>
</tr>
<tr>
<td>Distillation</td>
<td>Low-Rank Factorization</td>
</tr>
<tr>
<td>Compression</td>
<td>Finetuning</td>
</tr>
<tr>
<td>Hyperparameters</td>
<td>Encoding</td>
</tr>
<tr>
<td>Prompt</td>
<td>Benchmarks</td>
</tr>
<tr>
<td>Open Source Models</td>
<td>Agents</td>
</tr>
<tr>
<td>Memory</td>
<td></td>
</tr>
</tbody>
</table>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://therealm4rtin.github.io/blog/tags/papers/">Papers</a></li>
      <li><a href="https://therealm4rtin.github.io/blog/tags/state-of-the-art/">State-of-The-Art</a></li>
      <li><a href="https://therealm4rtin.github.io/blog/tags/llms/">LLMs</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://therealm4rtin.github.io/blog/posts/mechinterp/mechinterp/">
    <span class="title">« Prev</span>
    <br>
    <span>Mechanistic Interpretability</span>
  </a>
  <a class="next" href="https://therealm4rtin.github.io/blog/posts/build-gpus-rack/gpus/">
    <span class="title">Next »</span>
    <br>
    <span>GPUs is all you need</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://therealm4rtin.github.io/blog/">TheRealMartin&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
